{
 "cells": [
  {
   "source": [
    "# Natural Adversarial Observations for Atari RL \n",
    "\n",
    "With this notebook, you will be able to create natural-looking adversarial observations for Atari agents based on the DQN algorithm.\n",
    "This repository comes with 3 already trained agents for Enduro, Road Runner and Breakout."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "from agent.AtariAgent import DQN, ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Helper functions\n",
    "def preprocess(frame):\n",
    "    '''\n",
    "    Input: (210,160,3) image, pixel values between 0 and 255\n",
    "    Return: (210,160) grayscale image, pixel values between 0 and 255\n",
    "    '''\n",
    "    frame = frame.mean(2).astype(np.uint8)\n",
    "    return frame\n",
    "\n",
    "def frame_to_tensor(frame):\n",
    "    '''\n",
    "    Input: (210,160) grayscale image, pixel values between 0 and 255\n",
    "    Returns: (210,160) tensor of the image, pixel values between 0 and 1, moved to GPU\n",
    "    '''\n",
    "    frame = frame / 255.\n",
    "    tensor = torch.tensor(frame, dtype=torch.float32, device=device, requires_grad=False)\n",
    "    return tensor\n",
    "\n",
    "def deque_to_tensor(stack):\n",
    "    '''\n",
    "    Input: Deque of 4 (210,160) tensors, pixel values between 0 and 1\n",
    "    Output: Tensor of shape (1,4,210,160) for forward pass through the network\n",
    "    '''\n",
    "    stack = list(stack)\n",
    "    stack = torch.stack(stack).to(device)\n",
    "    return stack.unsqueeze(0)\n",
    "\n",
    "def random_transition(model, env, lower_bound=0, upper_bound=2000):\n",
    "    idx = random.randint(lower_bound, upper_bound)\n",
    "    state = env.reset()\n",
    "    transition = deque(maxlen=4)\n",
    "    for i in range(idx):\n",
    "        while len(transition) < 4:\n",
    "            state, _, done, _ = env.step(env.action_space.sample())\n",
    "            transition.append(frame_to_tensor(preprocess(state)))\n",
    "        state, _, done, _ = env.step(model.predict_action(deque_to_tensor(transition)))\n",
    "        transition.append(frame_to_tensor(preprocess(state)))\n",
    "        if done == True:\n",
    "            state=env.reset()\n",
    "    \n",
    "    return deque_to_tensor(transition)\n",
    "    \n",
    "\n",
    "def plot_images(images, n_rows, n_columns, figsize=(15,15), cmap='gray', grid=True, show=True, save=False, name=None):\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    columns = n_columns\n",
    "    rows = n_rows\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = images[i-1]\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "        if grid == False:\n",
    "            plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(name, dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_results(transition, delta, figsize=(15,15), cmap='gray', grid=True, show=True, save=False, name=None):\n",
    "    fig = plt.figure()\n",
    "    rows=1\n",
    "    columns = 4\n",
    "    plot_images(delta, n_rows=1, n_columns=delta.shape[0], figsize=figsize, grid=False, show=False)\n",
    "    if save:\n",
    "        plt.savefig(name + '_1.pdf', dpi=200)\n",
    "    plot_images(transition, n_rows=1, n_columns=transition.shape[0], figsize=figsize, grid=False, show=False)\n",
    "    if save:\n",
    "        plt.savefig(name + '_2.pdf', dpi=200)\n",
    "    plot_images(transition+delta, n_rows=1, n_columns=transition.shape[0], figsize=figsize, grid=False, show=False)\n",
    "    if save:\n",
    "        plt.savefig(name + '_3.pdf', dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#--Declare and fill memory for Enduro\n",
    "def fill_memory(env, model, memory, runs):\n",
    "    eps_reward = 0\n",
    "    frame_number = 0\n",
    "    done = False\n",
    "\n",
    "    state = env.reset()\n",
    "    states = deque(maxlen=4)\n",
    "    rewards = []\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    last_lives = 0\n",
    "    \n",
    "    \n",
    "    while memory.count < memory.size:\n",
    "        # stack frames\n",
    "        states.append(frame_to_tensor(preprocess(state)))\n",
    "        if len(states) < 4:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #as soon as 4 frames are stacked, use them to predict the action\n",
    "            stack = deque_to_tensor(states)\n",
    "            action = model.predict_action(stack)\n",
    "        # apply action to the environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        \n",
    "        if info['ale.lives'] < last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = done\n",
    "            last_lives = info['ale.lives']\n",
    "        \n",
    "        memory.add_experience(action=action,\n",
    "                   frame=preprocess(next_state),\n",
    "                   reward=reward, \n",
    "                   terminal=terminal_life_lost)\n",
    "        \n",
    "        \n",
    "        eps_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done == True:\n",
    "            state = env.reset()\n",
    "            print(\"Total episode reward: \", eps_reward)\n",
    "            rewards.append(eps_reward)\n",
    "            eps_reward = 0\n",
    "            done = False\n",
    "            if len(rewards) == runs:\n",
    "                break\n",
    "\n",
    "    print(\"Done!\")\n",
    "    avg_reward_unaltered = np.average(rewards)\n",
    "    print(\"Average reward over {} runs: {}\".format(len(rewards), avg_reward_unaltered))\n"
   ]
  },
  {
   "source": [
    "Depending on which agent you want to attack, first change the name of the game.\n",
    "\n",
    "Also, if you plan on attacking the agent for Enduro change the ```hidden``` parameter to '128' instead of '256' for Breakout and Road Runner.\n",
    "\n",
    "In the following cell, all actions and their meanings are printed to give you an overview of what the actions will most likely do if applied to the environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Create environment and load trained agent\n",
    "env = gym.make('Breakout-v4')\n",
    "n_actions = env.action_space.n\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "model = DQN(n_actions, hidden=256)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"agent/Breakout.pth\"))\n",
    "model.eval()\n",
    "\n",
    "!mkdir -p ./perturbations/Breakout/\n",
    "!mkdir -p ./memory/\n",
    "!mkdir -p ./plots/Breakout/diagrams/ics\n",
    "!mkdir -p ./plots/Breakout/diagrams/ucs\n",
    "!mkdir -p ./plots/Breakout/diagrams/cduap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Print action names\n",
    "print(env.unwrapped.get_action_meanings())\n"
   ]
  },
  {
   "source": [
    "Here we create and fill the replay buffer. The replay buffer is originally utilised during the training of the agents. It will also come in handy while creating the universal adversarial perturbations and to test the impact of our attack."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(size=30000)\n",
    "runs = 10\n",
    "\n",
    "fill_memory(env, model, memory, runs)\n",
    "# save the memory for later use\n",
    "#with open('./memory/Breakout.p','wb') as f: pickle.dump(memory, f)\n",
    "\n",
    "# or load an already saved replay buffer\n",
    "#memory = pickle.load(open('./memory/Breakout.p', 'rb'))"
   ]
  },
  {
   "source": [
    "## Input- and class-specific\n",
    "With this function, you will be able to create a perturbation for each input to the agent. The perturbation is generated such that the agent will only perform a single action (the action defined with ```target_action```)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Input and class specific\n",
    "def ics(model, transition, target_action, lr=3e-2, verbose=False, iterations=100):\n",
    "    assert target_action in range(n_actions), \"The target action must be in {}\".format(range(n_actions))\n",
    "    target_action = torch.LongTensor([target_action]).to(device)            # action needs to be a tensor for the optimizer\n",
    "    \n",
    "    delta = torch.zeros((4,210,160), requires_grad=True, device=device)     # create the perturbation to be the same size as the input and fill with 0s\n",
    "    opt = torch.optim.Adam([delta], lr=lr)                                  # initialize the optimizer to update the perturbation's pixel values with the \n",
    "                                                                            # set learning rate\n",
    "\n",
    "    pred_action = model.predict_action(transition).item()                   # get the 'original' action the agent will chosse for the input\n",
    "    \n",
    "    q_vals = model.predict_q(transition + delta)                            # get the predicted Q-values Q(s,a) for the adversarial example\n",
    "    \n",
    "    i = 0\n",
    "    while torch.argmax(q_vals) != target_action:                            # while the new, adversarial action is not equal to the desired target action\n",
    "        loss = F.cross_entropy(q_vals, target_action)                       # calculate the loss between the Q-values and the target action\n",
    "        opt.zero_grad()\n",
    "        loss.backward()                                                     # calculate the gradient w.r.t. the loss \n",
    "        opt.step()                                                          # and optimize the values of the perturbation\n",
    "        q_vals = model.predict_q(transition + delta)                        # get the predicted Q-values Q(s,a) for the adversarial example\n",
    "        i += 1\n",
    "        if i > iterations:                                                  # if a maximum amount of itertions is reached, break out of the function\n",
    "            break\n",
    "        if verbose:                                                         # if you want to see the progess, print it with the verbose option set to True\n",
    "            if i % 100 == 0:\n",
    "                 print(\"Epoch: {}, Loss: {}, Predicted action: {}\".format(i, loss, torch.argmax(logits)))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Action {} changed to action {} with perturbation\".format(pred_action, \\\n",
    "                                                                     model.predict_action(transition+delta).item()))\n",
    "    return delta                                                           # return the perturbation"
   ]
  },
  {
   "source": [
    "First, we will get a random input, a stack of 4 consecutive, grayscaled frames from the environment, in this case called transition. \n",
    "Of course, we need a transition for which the agent will predict a different action than the one we want it to predict after the attack. \n",
    "Here, we set the target action to be action 0 and get a random transition from the environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_action = pred = 0                                                    # set the desired target action\n",
    "while pred == target_action:\n",
    "    transition = random_transition(model, env)                              # and get a random transition from the environment\n",
    "    pred = model.predict_action(transition).item()\n",
    "\n",
    "delta_ics = ics(model, transition, target_action=target_action, lr=5e-3, iteration=100, verbose=True)   # get the perturbation with the defined function \n",
    "                                                                                                        # from above\n",
    "#with open('perturbations/Breakout/ics','wb') as f: pickle.dump(delta_ics.detach().cpu().numpy(), f)    # If you want to save the perturbation, uncomment\n",
    "#plt.imshow(200*delta_is_cs.detach().cpu().permute(1,2,0).numpy())                                      # If you want to save an image of the perturbation,\n",
    "                                                                                                        # uncomment the next lines\n",
    "#plt.axis('off')\n",
    "#plt.savefig('perturbations/Breakout/ics.png', dpi=200)  \n",
    "plot_results(transition.detach().squeeze(0).cpu().numpy(), delta_is_cs.detach().cpu().numpy(), save=False, name='plots/Breakout/ics_2')   # plot the results,\n",
    "                                                                                                                                        # save them to the \n",
    "                                                                                                                                        # desired location\n",
    "print(\"Delta in range: ({},{})\".format(torch.min(delta_is_cs.data).item(), torch.max(delta_is_cs.data).item()))                         # print just for \n",
    "                                                                                                                                        # checking the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to print and save the perturbation\n",
    "transition_full = transition.squeeze(0)[:4].detach().cpu().permute(1,2,0).numpy()                           \n",
    "delta_full = delta_ics[:4].detach().cpu().permute(1,2,0).numpy()\n",
    "plot_images((transition_full, 200*delta_full, transition_full+delta_full), n_rows=1, n_columns=3, grid=False, save=False, name='plots/Breakout/ics_1.pdf') # set save to True if you want to save the image\n",
    "action_ori = model.predict_action(transition[0].unsqueeze(0))\n",
    "action_delta = model.predict_action(delta_ics.unsqueeze(0))\n",
    "action_perturbed = model.predict_action((transition[0]+delta_ics).unsqueeze(0))\n",
    "print(\"\\t    Action {}\\t\\t\\t       Action {}\\t\\t\\t\\t   Action {}\".format(action_ori.item(), action_delta.item(), action_perturbed.item()))"
   ]
  },
  {
   "source": [
    "Here we declare the test procedure for the perturbation. The goal is to find perturbations, that will impact the agent to perform not worse than -50% compared to an unattacked agent. Otherwise, the attack will most likely be detected. For mor details, take a look at the 'Result' chapter of the thesis.\n",
    "\n",
    "By lowering the pixel values of the perturbation to certain percentages, we approximate the calculated attack if their pixel values were restricted to stay in \\[-epsilon \\* intensity, epsilon \\* intensity\\]. For the ICS attack, the pixel values are not explicitly restricted, since the attack creates usually very tiny perturbations, but for the following attacks, the epsilon is initially set.\n",
    "\n",
    "In the case of the ICS attack, the perturbation is calculated for each transition stored in the memory. Additionally, the original ('correct' -- although one does never know what the true correct action is in RL) action the agent predicted for it is stored in the memory and will be utilised for comparison.\n",
    "Now the adversarial example will be generated with the above function. The pixel values of this calculated perturbation are the ones at 100% intensity. Now we let the agent predict an action for all perturbations at 100%, 75%, 50%, 25% and 5% and compare it to the original, stored action. If the predicted action equals the original one, the counter in the corresponding list is incremented. Later we calculate the 'agreement rate' -- how many actions were still equal to the original ones after the attack in percent. A low agreement rate on all actions except the one we want the agent to predict (in the above case 0\n",
    "is good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation(mode, perturbation=None, runs=10, target_action=None):\n",
    "\n",
    "    if mode not in ['ics', 'ucs', 'cduap']:                                                     # choose the attack from ICS, UCS or CDUAP\n",
    "        print(\"Please select one of these methods: 'ics', 'ucs', 'cduap'\")\n",
    "        return\n",
    "    if mode == 'ics' and target_action == None:\n",
    "        print(\"Choose an action to shift to with the perturbation!\")\n",
    "        print(\"Possible actions for the current game: \", range(n_actions))\n",
    "        return\n",
    "    if mode in ['ucs', 'cduap'] and perturbation==None:\n",
    "        print(\"Test routine needs a perturbation to work on!\")\n",
    "        return\n",
    "        \n",
    "\n",
    "\n",
    "    still_correct_100 = [0] * n_actions                                                         # initialize lists to keep track of how many actions were\n",
    "    still_correct_75 = [0] * n_actions                                                          # still predicted 'correctly'\n",
    "    still_correct_50 = [0] * n_actions\n",
    "    still_correct_25 = [0] * n_actions\n",
    "    still_correct_5 = [0] * n_actions\n",
    "\n",
    "    agreement_rate_100 = [0] * n_actions\n",
    "    agreement_rate_75 = [0] * n_actions\n",
    "    agreement_rate_50 = [0] * n_actions\n",
    "    agreement_rate_25 = [0] * n_actions\n",
    "    agreement_rate_5 = [0] * n_actions\n",
    "\n",
    "    amount = [0] * n_actions\n",
    "\n",
    "    for i in range(memory.count-4):                                                                                                                                       \n",
    "        current_transition = torch.cuda.FloatTensor(memory._get_state(i+3)/255.)                # get the next transition from the memory\n",
    "        correct = memory.actions[i]                                                             # and the originally stored action\n",
    "        amount[correct] += 1                                                                    # save the quantity of these actions for later\n",
    "        \n",
    "        if mode == 'ics':\n",
    "            perturbation = ics(model, current_transition.unsqueeze(0), target_action=target_action, lr=5e-4)    # calculate the ICS perturbation, for UCS \n",
    "                                                                                                # and CD-UAP, the perturbation is handed over to the \n",
    "                                                                                                # function in the beginning\n",
    "\n",
    "        prediction_100 = model.predict_action((current_transition+perturbation).unsqueeze(0))           # predict actions for all intensities\n",
    "        prediction_75 = model.predict_action((current_transition+(perturbation*0.75)).unsqueeze(0))\n",
    "        prediction_50 = model.predict_action((current_transition+(perturbation*0.5)).unsqueeze(0))\n",
    "        prediction_25 = model.predict_action((current_transition+(perturbation*0.25)).unsqueeze(0))\n",
    "        prediction_5 = model.predict_action((current_transition+(perturbation*0.05)).unsqueeze(0))\n",
    "\n",
    "        if prediction_100 == correct:                                                           # increase the counter, if the prediction quals the original \n",
    "                                                                                                # action\n",
    "            still_correct_100[correct] += 1\n",
    "        if prediction_75 == correct:\n",
    "            still_correct_75[correct] += 1\n",
    "        if prediction_50 == correct:\n",
    "            still_correct_50[correct] += 1\n",
    "        if prediction_25 == correct:\n",
    "            still_correct_25[correct] += 1\n",
    "        if prediction_5 == correct:\n",
    "            still_correct_5[correct] += 1\n",
    "\n",
    "    for i in range(n_actions):\n",
    "        agreement_rate_100[i] = (still_correct_100[i]/amount[i])*100                            # calculate the agreement rate in percent\n",
    "        agreement_rate_75[i] =  (still_correct_75[i]/amount[i])*100\n",
    "        agreement_rate_50[i] =  (still_correct_50[i]/amount[i])*100\n",
    "        agreement_rate_25[i] =  (still_correct_25[i]/amount[i])*100\n",
    "        agreement_rate_5[i] =   (still_correct_5[i]/amount[i])*100\n",
    "\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return agreement_rate_100, agreement_rate_75, agreement_rate_50, agreement_rate_25, agreement_rate_5  # return the rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the agreement rates and print them for later use\n",
    "agreement_rate_100, agreement_rate_75, agreement_rate_50, agreement_rate_25, agreement_rate_5 = test_perturbation('ics', target_action=0)\n",
    "print(agreement_rate_100)\n",
    "print(agreement_rate_75)\n",
    "print(agreement_rate_50)\n",
    "print(agreement_rate_25)\n",
    "print(agreement_rate_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a diagramm and save it\n",
    "#plt.figure(figsize=(5, 3))\n",
    "#plt.bar(env.unwrapped.get_action_meanings(), accuracy_drop_5)\n",
    "#plt.axis([-1, n_actions, 0, 105])\n",
    "\n",
    "#plt.savefig('./plots/Breakout/diagrams/ics/accuracy_drop_5.pdf', dpi=200)\n",
    "#plt.show()"
   ]
  },
  {
   "source": [
    "## Universal, class specific attack\n",
    "The universal and class specific attack is calculated on the whole memory we filled earlier. Since every perturbation is calculated on the same memory during the tests, they can be compared to each other.\n",
    "\n",
    "The main idea resembles the ICS attack. You can choose an action that the agent should always predict. Now we get a minibatch of random transitions. The perturbations pixel values are updated to minimise the loss between the agent's predictions and your chosen target action. Since the perturbation is now calculated to manipulate multiple inputs simultaneously, it might be visible in the final adversarial example. Therefore, the values are restricted to stay in \\[-epsilon,epsilon\\]."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucs(model, env, memory, target_action, epochs=400, lr=3e-3, batch_size=32, epsilon=5e-3, verbose=True):\n",
    "    target_action = torch.ones(batch_size).to(device) * target_action                           # create a tensor of size [batch_size], containing only\n",
    "    target_action = target_action.long()                                                        # the target_action\n",
    "\n",
    "    delta = torch.zeros((4,210,160), requires_grad=True, device=device)                         # initialize perturbation\n",
    "    opt = torch.optim.Adam([delta], lr=lr)                                                      # and optimizer\n",
    "\n",
    "    for i in range(epochs):                                                 \n",
    "        states, _, _, _, _ = memory.get_minibatch()                                             # get a random sample of inputs\n",
    "        q_vals = model.predict_q(torch.cuda.FloatTensor(states/255.) + delta)                   # predict the Q-values for the adversarial examples\n",
    "        loss = F.cross_entropy(q_vals, target_action)                                           # calculate the loss\n",
    "\n",
    "        if verbose:                                                                             # print some information if you want to\n",
    "            if i % (epochs/10) == 0:\n",
    "                print(\"Epoch: {}, Loss: {}, Predicted actions: {}\".format(i, loss, torch.argmax(q_vals, dim=1).cpu().numpy()))\n",
    "\n",
    "        opt.zero_grad() \n",
    "        loss.backward()                                                                         # calculate the gradient w.r.t. the loss\n",
    "        opt.step()                                                                              # update the perturbation\n",
    "\n",
    "        delta.data.clamp_(-epsilon, epsilon)                                                    # clip the pixel values to stay in certain range\n",
    "\n",
    "\n",
    "    plot_results(states[0], delta.detach().cpu().numpy(), save=False, name='plots/Breakout/ucs_2_'+str(target_action[0].item())) # plot the images, save if needed\n",
    "    \n",
    "\n",
    "    print(\"Action {} changed to action {} with perturbation.\".format(model.predict_action(torch.cuda.FloatTensor(states[0]/255.).unsqueeze(0)).item(), \\\n",
    "                                                                     model.predict_action((torch.cuda.FloatTensor(states[0]/255.)+delta).unsqueeze(0)).item()))\n",
    "\n",
    "\n",
    "\n",
    "                 \n",
    "    print(\"Delta in range: ({},{})\".format(torch.min(delta.data).item(), torch.max(delta.data).item()))\n",
    "    return states, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions, delta_ucs = ucs(model, env, memory, 0, epochs=100, epsilon=7e-3, lr=3e-4)\n",
    "#with open('perturbations/Breakout/ucs_0','wb') as f: pickle.dump(delta_ucs.detach().cpu().numpy(), f)          # uncomment these lines if you want to save\n",
    "#plt.imshow(200*delta_ucs.detach().cpu().permute(1,2,0).numpy())                                                # the perturbation and an image of it\n",
    "#plt.axis('off')\n",
    "#plt.savefig('perturbations/Breakout/ucs_0.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to print and save the perturbation\n",
    "transition_full = transitions[0]/255.\n",
    "delta_full = delta_ucs.detach().cpu().permute(1,2,0).numpy()\n",
    "plot_images((np.transpose(transition_full, axes=(1,2,0)), 200*delta_full, np.transpose(transition_full, axes=(1,2,0))+delta_full), n_rows=1, n_columns=3, grid=False, save=False, name='plots/Breakout/ucs_1.pdf')                                            # set save to True if you want to save these images\n",
    "action_ori = model.predict_action(torch.cuda.FloatTensor(transition_full).unsqueeze(0))\n",
    "action_delta = model.predict_action(delta_ucs.unsqueeze(0))\n",
    "action_perturbed = model.predict_action((torch.cuda.FloatTensor(transition_full)+delta_ucs).unsqueeze(0))\n",
    "print(\"\\t    Action {}\\t\\t\\t       Action {}\\t\\t\\t\\t   Action {}\".format(action_ori.item(), action_delta.item(), action_perturbed.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the agreement rates and print them for later use\n",
    "agreement_rate_100, agreement_rate_75, agreement_rate_50, agreement_rate_25, agreement_rate_5 = test_perturbation('ucs', perturbation=delta_ucs)\n",
    "print(agreement_rate_100)\n",
    "print(agreement_rate_75)\n",
    "print(agreement_rate_50)\n",
    "print(agreement_rate_25)\n",
    "print(agreement_rate_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a diagramm and save it\n",
    "#plt.figure(figsize=(5, 3))\n",
    "#plt.bar(env.unwrapped.get_action_meanings(), accuracy_drop_5)\n",
    "#plt.axis([-1, n_actions, 0, 105])\n",
    "#plt.savefig('./plots/Breakout/diagrams/ucs/accuracy_drop_5.pdf', dpi=200)\n",
    "#plt.show()"
   ]
  },
  {
   "source": [
    "## CD-UAP for Atari games\n",
    "The CD-UAP was originally introduced by [Zhang et al.](https://doi.org/10.1609/AAAI.V34I04.6154) and now modified and applied to RL and Atari games.\n",
    "\n",
    "The idea is to lower the agent's confidence on predicting certain actions (target actions) while not creating too much impact on the others from the action space (not-targeted actions). To achieve this, the loss term needs to consists of two seperate loss functions. One that reduces the loss between the predictions for all adversarial examples of the corresponding not-targetd inputs; and one that shifts the predictions from a targeted class to another.\n",
    "\n",
    "### Example\n",
    "For Enduro, we set the targeted actions to be (1,7,8), so whenever the agent would choose action 1 for the original input, it chooses any other action for the adversarial example instead. On the other hand, if it chooses action 2 for the original input, the action 2 will be chosen for the final adversarial example as well.\n",
    "\n",
    "\n",
    "First, we get two equally-sized batches of random sampled, trageted or not-targeted transitions. We then save the originial, stored actions of the not-targeted inputs. These are used to minimize the loss between the predicted Q-values and the 'correct' actions. Afterwards, the second loss term is calculated. For that the best and second best action are calculated for the originial input. Select the Q-value for the adversarial example at the index of the best action ($L_c$) and the Q-value at the index of the second best action ($L_i$). We want to shift the prediction towards the second best guess, away from the original action, which is why $L_{nt} = L_c - L_i$. Finally, values are updated on both loss terms $L_{t}$ and $L_{nt}$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_uap(model, env, memory, target_actions, epochs=100, lr=3e-3, batch_size=32, epsilon=5e-3, verbose=True):\n",
    "    #target_action = torch.LongTensor(target_actions)\n",
    "\n",
    "    delta = torch.zeros((4,210,160), requires_grad=True, device=device)\n",
    "    opt = torch.optim.Adam([delta], lr=lr)\n",
    "\n",
    "    for i in range(epochs):     \n",
    "        frames_t = deque(maxlen=int(batch_size/2))                                          # initialize the containers (deques) for the targeted transitions,\n",
    "        frames_nt = deque(maxlen=int(batch_size/2))                                         # not-targeted inputs,\n",
    "        actions_nt = deque(maxlen=int(batch_size/2))                                        # and not-targeted, original action\n",
    "    \n",
    "        \n",
    "        while (len(frames_t) < int(batch_size/2)) or (len(frames_nt) < int(batch_size/2)):  # fill both input containers\n",
    "            states, actions, _, _, _ = memory.get_minibatch()\n",
    "            states = torch.cuda.FloatTensor(states/255.)\n",
    "            actions = torch.cuda.LongTensor(actions)\n",
    "\n",
    "            for z in range(len(states)):\n",
    "                if actions[z] in target_actions:\n",
    "                    frames_t.append(states[z])\n",
    "                else:\n",
    "                    frames_nt.append(states[z])\n",
    "                    actions_nt.append(actions[z])\n",
    "\n",
    "                    \n",
    "        q_vals_nt = model.predict_q(torch.stack(list(frames_nt) + delta).to(device))                # calculate the Q-values for the not-targeted adversarial examples\n",
    "        loss_nt = F.cross_entropy(q_vals_nt, torch.stack(list(actions_nt)).to(device))              # calculate the loss for the not-targeted examples\n",
    "\n",
    "        q_vals_t = model.predict_q(torch.stack(list(frames_t)).to(device))                          # calculate the Q-values for the originial targeted examples\n",
    "        c, _i = torch.split(torch.topk(q_vals_t, 2)[1], (1,1), dim=1)                               # get the best and second best action from the Q-values\n",
    "        L_c = model.predict_q(torch.stack(list(frames_t)).to(device) + delta).gather(1, c)          # calculate the Q-value at the index of the best action for the targeted \n",
    "                                                                                                    # adversarial examples\n",
    "        L_i = model.predict_q(torch.stack(list(frames_t)).to(device) + delta).gather(1, _i)         # calculate the Q-value at the index of the best action for the \n",
    "                                                                                                    # not-targeted adversarial examples\n",
    "        loss_t = L_c - L_i                                                                          # calculate the loss for the targeted examples\n",
    "        for x in range(len(loss_t)):                                                                # set to 0 if loss gets negative\n",
    "            if loss_t[x] < 0:\n",
    "                loss_t[x] = 0.\n",
    "        loss_t = torch.sum(loss_t)\n",
    "    \n",
    "\n",
    "        loss = (loss_t + loss_nt)                                                                   # add loss terms to single loss\n",
    "\n",
    "        if verbose:                                                                                 # print some information during training\n",
    "            if i % int(epochs/10) == 0:\n",
    "                print(\"Epoch: {}, Loss: {}, Predicted actions: {}\".format(i, loss, torch.argmax(q_vals_t, dim=1).cpu().numpy()))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()                                                                             # calculate the gradient w.r.t. the loss\n",
    "        opt.step()                                                                                  # update the values of the perturbations\n",
    "\n",
    "        delta.data.clamp_(-epsilon, epsilon)                                                        # restrict the values of the perturbation\n",
    "\n",
    "\n",
    "    plot_results(frames_t[0].detach().squeeze(0).cpu().numpy(), delta.detach().cpu().numpy(), save=False, name='plots/Breakout/cduap_2')# plot results and set save if necessary\n",
    "    #with open('perturbations/Breakout/cduap','wb') as f: pickle.dump(delta.detach().cpu().numpy(), f)  # uncomment these lines if you want to save the perturbation and\n",
    "    #plt.imshow(200*delta.detach().cpu().permute(1,2,0).numpy())                                        # an image of it\n",
    "    #plt.axis('off')\n",
    "    #plt.savefig('perturbations/Breakout/cduap.png', dpi=200)\n",
    "\n",
    "    print(\"Action {} changed to action {} with perturbation.\".format(model.predict_action(frames_t[0].unsqueeze(0)).item(), \\\n",
    "                                                                     model.predict_action((frames_t[0]+delta).unsqueeze(0)).item()))\n",
    "\n",
    "    print(\"Delta in range: ({},{})\".format(torch.min(delta.data).item(), torch.max(delta.data).item()))\n",
    "    return frames_t, delta                                                                          # return the perturbation and the batch of targeted inputs (for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions, delta_cd_uap = cd_uap(model, env, memory, (2,3), epochs=400, epsilon=7e-3, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to print and save the perturbation\n",
    "transition_full = transitions[0].detach().cpu().permute(1,2,0).numpy()\n",
    "delta_full = delta_cd_uap.detach().cpu().permute(1,2,0).numpy()\n",
    "plot_images((transition_full, 200*delta_full, transition_full+delta_full), n_rows=1, n_columns=3, grid=False, save=True, name='plots/Breakout/cduap_1.pdf')\n",
    "action_ori = model.predict_action(transitions[0].unsqueeze(0))\n",
    "action_delta = model.predict_action(delta_cd_uap.unsqueeze(0))\n",
    "action_perturbed = model.predict_action((transitions[0]+delta_cd_uap).unsqueeze(0))\n",
    "print(\"\\t    Action {}\\t\\t\\t       Action {}\\t\\t\\t\\t   Action {}\".format(action_ori.item(), action_delta.item(), action_perturbed.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the agreement rates and print them for later use\n",
    "agreement_rate_100, agreement_rate_75, agreement_rate_50, agreement_rate_25, agreement_rate_5 = test_perturbation('cduap', perturbation=delta_cd_uap)\n",
    "print(agreement_rate_100)\n",
    "print(agreement_rate_75)\n",
    "print(agreement_rate_50)\n",
    "print(agreement_rate_25)\n",
    "print(agreement_rate_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a diagramm and save it\n",
    "#plt.figure(figsize=(5, 3))\n",
    "#plt.bar(env.unwrapped.get_action_meanings(), accuracy_drop_5)\n",
    "#plt.axis([-1, n_actions, 0, 105])\n",
    "\n",
    "\n",
    "#plt.savefig('./plots/Breakout/diagrams/cduap/accuracy_drop_5.pdf', dpi=200)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}